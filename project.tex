\documentclass[11pt]{article}
\usepackage[margin=1in]{geometry}
\usepackage{amsmath, amssymb, amsthm}
\usepackage{algorithm}
\usepackage{algpseudocode}
\usepackage{hyperref}
\usepackage{booktabs}
\usepackage{enumitem}

\title{\textbf{Learning to Learn: Curriculum Selection via Reinforcement Learning for Sample-Efficient Language Models}}
\author{Scott Viteri}
\date{}

\begin{document}
\maketitle

\section{Overview}

We propose an algorithm that enables language models to learn what data to learn from. Rather than training on randomly sampled data, the model takes actions that select training examples, receiving reward based on how much each selection improves prediction on held-out data. The core hypothesis is that learned curriculum selection will yield significantly better sample efficiency than random or heuristic curricula.

\section{Technical Approach}

\subsection{Setup}

\begin{itemize}[nosep]
    \item Language model (GPT-2) with parameters $\theta$
    \item Off-the-shelf embedding model $\phi \colon \text{text} \to \mathbb{R}^{d_\phi}$ for encoding streaming data (e.g.\ sentence-transformers)
    \item Streaming data source, partitioned into training candidates and held-out evaluation
    \item Value vector $w \in \mathbb{R}^{d_\phi}$, living in the same embedding space as $\phi$
    \item Boltzmann temperature $\beta > 0$ controlling exploration
    \item Learning rate $\alpha$ for the LM gradient step; learning rate $\eta$ for the value regression step
    \item Reward scaling factor $c$ (raw rewards $r_k \sim O(10^{-3})$ are multiplied by $c$ before value regression)
    \item Gradient clipping threshold $G$ (max global norm; all LM gradient steps are clipped to $\|\nabla\| \le G$)
\end{itemize}

\subsection{Data Streaming}\label{sec:streaming}

Data streams in continuously. At each outer step, a fresh batch of context windows arrives and is partitioned:
\begin{itemize}[nosep]
    \item $\sim$900 become training candidates (the set $S_t$ at outer step $t$)
    \item $\sim$400 are reserved for held-out evaluation (the set $D_t$ at outer step $t$)
\end{itemize}

Each incoming data point $x$ is embedded via an off-the-shelf embedding model: $e_x = \phi(x)$. These embeddings are used for the selection mechanism described below.

Each element of $S_t$ or $D_t$ is conceptually one full transformer context window of text.

\subsection{Algorithm}

The algorithm alternates two update steps---coordinate descent on the free energy.  At each outer-loop stream step $t$, we refresh $S_t, D_t$ from newly arrived data and hold them fixed for the duration of the inner loop.  We then run $K$ inner-loop steps on fixed $S_t, D_t$.

The policy over training candidates is the Boltzmann distribution:
\begin{equation}
    \pi(x) = \operatorname{softmax}\!\Bigl(\frac{w \cdot \phi(x)}{\beta}\Bigr), \qquad x \in S_t,
\end{equation}
where $w \in \mathbb{R}^{d_\phi}$ is a learned value vector in embedding space and $\beta$ is the temperature.  The estimated value of candidate $x$ is $V(x) = w \cdot \phi(x)$.

At initialisation, $w = 0$, so the policy is uniform over $S_t$.  The value vector is updated by online linear regression toward observed rewards, causing the policy to concentrate on candidates predicted to yield high held-out improvement.

Each inner step $k$ proceeds as follows:

\begin{enumerate}[nosep]
    \item \textbf{Select a training example (E-step).}
    Sample $x_k \sim \pi$ from the candidates $S_t$ using the current value vector $w$.

    \item \textbf{Language-modeling update (M-step).}
    \begin{equation}
        \theta_{k+1} = \theta_k + \alpha\, \nabla_{\theta_k} \log p_{\theta_k}(x_k).
    \end{equation}
    This is a standard gradient ascent step on the log-likelihood of the selected example.  Note that we only need to cache the held-out log-probability $\frac{1}{|D_t|}\log p_{\theta_k}(D_t)$ from the previous step, not the full parameter snapshot $\theta_k$.

    \item \textbf{Reward.}
    The reward is the one-step held-out improvement:
    \begin{equation}
        r_k = \frac{1}{|D_t|}\log p_{\theta_{k+1}}(D_t) - \frac{1}{|D_t|}\log p_{\theta_k}(D_t).
    \end{equation}
    In practice, $r_k$ is scaled by a constant $c$ (since raw $r_k \sim O(10^{-3})$) before the value update.

    \item \textbf{Value vector regression.}
    Update $w$ by online linear regression toward the observed (scaled) reward:
    \begin{equation}
        w \;\leftarrow\; w - \eta\,\bigl(w \cdot \phi(x_k) - c\, r_k\bigr)\,\phi(x_k).
    \end{equation}
    This drives the predicted value $w \cdot \phi(x_k)$ toward the observed reward, so that future sampling via $\pi$ favours candidates whose embeddings are similar to those that previously yielded high held-out improvement.
\end{enumerate}

\noindent\textbf{Key simplifications.}  This formulation requires no Q-function, no value baseline, no Bellman loss, no parameter snapshots, and no backpropagation through the selection policy.  The value vector $w$ is updated by a closed-form regression step that is decoupled from the LM optimiser.  The only neural-network gradient computation is the standard LM training step (step~2).

\subsection{Interpretation as Free-Energy Minimisation}

The Boltzmann policy $\pi(x) \propto \exp(w \cdot \phi(x) / \beta)$ can be viewed as the solution to a free-energy minimisation problem:
\begin{equation}
    \pi^* = \arg\min_\pi \;\Bigl[\; -\mathbb{E}_{x \sim \pi}\bigl[V(x)\bigr] + \beta\, H(\pi) \;\Bigr],
\end{equation}
where $H(\pi)$ is the entropy of the selection distribution.  The first term encourages selecting high-value candidates; the second (controlled by $\beta$) encourages exploration.  The value regression step (step~4) updates $V(x) = w \cdot \phi(x)$ to better predict which candidates yield held-out improvement, and the Boltzmann form automatically adjusts the policy accordingly.

\section{Evaluation}

\subsection{Primary Metric: Sample Efficiency}

The central question is: how many training examples does the model need to reach a given performance level?

We will measure:
\begin{itemize}[nosep]
    \item Perplexity on a fixed evaluation set as a function of training examples seen
    \item Performance on downstream tasks (e.g., MMLU, HellaSwag) as a function of training examples
    \item Comparison to human learning efficiency as an aspirational benchmark
\end{itemize}

\subsection{Baselines}

\begin{itemize}[nosep]
    \item \textbf{Random curriculum:} Uniform sampling from candidates
    \item \textbf{Loss-based curriculum:} Prioritize high-loss examples
    \item \textbf{Uncertainty-based curriculum:} Prioritize examples with high model uncertainty
    \item \textbf{Competence-based curriculum:} Examples ordered by difficulty (requires difficulty labels)
\end{itemize}

\subsection{Analysis}

Beyond aggregate metrics, we will examine:
\begin{itemize}[nosep]
    \item \textbf{Curriculum structure:} Does the learned curriculum exhibit interpretable patterns? Developmental stages? Topic clustering?
    \item \textbf{Exploration dynamics:} How does the policy entropy evolve over training? Does the Boltzmann temperature $\beta$ produce reasonable exploration?
    \item \textbf{Value vector interpretability:} What directions does $w$ learn? Can we understand what makes a training example ``valuable'' by examining $w \cdot \phi(x)$ across candidates?
\end{itemize}

\section{Relation to Prior Work}

\subsection{Curriculum Learning}

Graves et al.\ (2017) use multi-armed bandits to select tasks; Jiang et al.\ (2018) train a separate ``mentor'' network to weight examples. Our approach differs by embedding selection in the model's forward pass and using RL to optimize for held-out improvement directly.

\subsection{Meta-Learning}

MAML (Finn et al., 2017) learns initializations for fast adaptation. We share the computational structure---reasoning about the effect of gradient updates---but learn \emph{what} to train on rather than \emph{where} to start.

\subsection{Intrinsic Motivation}

Our objective relates to compression progress (Schmidhuber) and active inference, but avoids the memory requirements of the former and the dark room problem of the latter by using streaming held-out data as a proxy for predictive success.

\subsection{Connection to My Prior Work}

This proposal builds directly on my recent work on Markovian Transformers for Informative Language Modeling (\url{https://arxiv.org/abs/2404.18988}). That work introduces a framework for training language models to generate Chain-of-Thought reasoning that is \emph{causally load-bearing}---the CoT must contain all information needed to predict the answer, as the model cannot attend back to the original question.

Both projects share a common structure: use RL to learn intermediate representations that improve prediction on held-out data. In the Markovian Transformers work, we learn CoTs:
\[
\text{Question} \to \text{CoT} \to \text{Answer}
\]
In the current proposal, we learn curricula:
\[
\text{Model State} \to \text{Selected Data} \to \text{Improved Predictions}
\]

The Markovian Transformers work demonstrates that this general approach is tractable and yields large gains on reasoning benchmarks (e.g., GSM8K: 19.6\% $\to$ 57.1\%). The current proposal extends this framework from learning \emph{what to say} to learning \emph{what to study}.

\section{Broader Motivation}

Language models trained to predict text develop remarkable capabilities from a simple objective. Yet they require extensive post-training to behave agentically and arguably lack a kind of global coherence. One hypothesis: the training process is purely observational---the model never takes actions that affect what it observes.

This project is a stepping stone toward studying whether learned curriculum selection produces qualitatively different agents. The immediate goal is demonstrating sample efficiency gains. The longer-term question is whether controlling one's own learning process contributes to the coherence and agency that current models seem to lack.

\subsection{Long-Term Direction: Formalizing Homeostasis}

Biological agents are shaped by survival pressures. Hunger, pain, and fatigue are not arbitrary reward signals---they are tied to the organism's continued existence. Current approaches to intrinsic motivation (curiosity, empowerment, compression progress) capture aspects of adaptive behavior but lack this grounding in self-preservation.

A long-term goal of this research program is to formalize homeostasis and survival into a simple, biologically plausible metric that could serve as a foundation for intrinsic motivation in artificial systems. The current project---learning to select data that improves future prediction---is a minimal step in this direction: the agent takes actions that maintain its predictive capacity, a kind of epistemic homeostasis. Understanding what this simple case yields will inform more ambitious formulations.

\section{Timeline and Resources}

\begin{table}[h]
\centering
\begin{tabular}{@{}ll@{}}
\toprule
\textbf{Phase} & \textbf{Duration} \\
\midrule
Infrastructure and baseline implementation & 6 weeks \\
Core algorithm implementation and debugging & 8 weeks \\
Initial experiments and hyperparameter tuning & 6 weeks \\
Scaling experiments on larger models & 4 weeks \\
Analysis and writeup & 4 weeks \\
\midrule
\textbf{Total} & \textbf{6 months} \\
\bottomrule
\end{tabular}
\end{table}

\subsection{Compute Resources}

\begin{itemize}[nosep]
    \item \textbf{Initial experiments:} Stanford Sherlock cluster (covered by existing allocation)
    \item \textbf{Final scaling experiments:} Cloud compute (Runpod or similar), approximately \$10K for H100/H200 time to validate results on larger models (1B+ parameters)
\end{itemize}

\section{Expected Outcomes}

\begin{enumerate}[nosep]
    \item Empirical demonstration of sample efficiency gains (or well-documented negative result)
    \item Open-source implementation of the algorithm
    \item Analysis of learned curriculum structure
    \item Foundation for future work on agency, homeostasis, and learned exploration in language models
\end{enumerate}

\section{About the Applicant}

Scott Viteri is a final-year PhD student at Stanford working on machine learning and reinforcement learning, with a focus on continual learning in transformers. His relevant prior work includes:

\begin{itemize}[nosep]
    \item \textbf{Markovian Transformers for Informative Language Modeling} (\url{https://arxiv.org/abs/2404.18988}): Introduces a framework for training language models with RL to produce causally load-bearing Chain-of-Thought reasoning. Demonstrates large gains on reasoning benchmarks (GSM8K, ARC-Challenge, MMLU). This work establishes the technical foundation for the current proposal.
    \item \textbf{Epistemic Phase Transitions in Mathematical Proofs} (Cognition, 2022): Studies how belief formation operates in mathematical reasoning using network models, showing how modular structure and bidirectional inference enable certainty to emerge despite local error rates.
    \item \textbf{ARC Eliciting Latent Knowledge Prize winner}: Recognized for work on methods to extract truthful information from language models.
    \item Experience with category-theoretic approaches to machine learning, collaborating with researchers at the Topos Institute.
\end{itemize}

\end{document}
